{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import where\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 2\n",
    "\n",
    "## Useful formula\n",
    "\n",
    "I have written out the folowing formula to help with the work of this worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X: np.array) -> float:\n",
    "    return - (X * where(X != 0, np.log2(X), 0)).sum()\n",
    "\n",
    "def joint_entropy(D: np.ndarray) -> float:\n",
    "    return - (D * where(D != 0, np.log2(D), 0)).sum()\n",
    "\n",
    "def conditional_entropy(D: np.ndarray, Y: np.array) -> float:\n",
    "    return joint_entropy(D) - entropy(Y)\n",
    "\n",
    "def mutual_information(D: np.ndarray) -> float:\n",
    "    return entropy(D.sum(0)) + entropy(D.sum(1)) - joint_entropy(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: marginal and conditional probabilities\n",
    "\n",
    "Work out the marginal probability distributions and the $x=a$ conditional probability distribution $P(Y\\,|\\,X=a) for:\n",
    "\n",
    "| Y \\ X | a | b |\n",
    "| ---- | ---- | ---- |\n",
    "| 1 | $\\frac{1}{3}$ | $\\frac{1}{6}$ |\n",
    "| 2 | $0$ | $\\frac{1}{4}$ |\n",
    "| 3 | $\\frac{1}{8}$ | $\\frac{1}{8}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.25      , 0.25      , 0.45833333, 0.54166667])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the distribution\n",
    "\n",
    "D = np.array([\n",
    "    [1/3, 1/6],\n",
    "    [0, 1/4],\n",
    "    [1/8, 1/8]\n",
    "])\n",
    "\n",
    "# Marginal distributions in the order [1,2,3,'a','b']\n",
    "\n",
    "np.concatenate((D.sum(1), D.sum(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72727273, 0.        , 0.27272727])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conditional probability, given X=a\n",
    "\n",
    "given_X = D[:,0]\n",
    "\n",
    "given_X / given_X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: working out entropy\n",
    "\n",
    "Each throw has a $\\displaystyle\\frac{1}{2}$ chance of being heads, constructing a table:\n",
    "\n",
    "| throws | probability |\n",
    "| ---- | ---- |\n",
    "| 1 | $\\frac{1}{2}$ |\n",
    "| 2 | $\\frac{1}{4}$ |\n",
    "| 3 | $\\frac{1}{8}$ |\n",
    "| 4 | $\\frac{1}{16}$ |\n",
    "| 5 | $\\frac{1}{32}$ |\n",
    "| ... | ... |\n",
    "\n",
    "As you can see from the code below as $\\text{throws} \\rightarrow \\infty$ then $H(X) = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2a.\n",
    "\n",
    "D = np.array([1/2**x for x in range(1, 500)])\n",
    "\n",
    "entropy(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more theoretical answer, the set of possible outcomes is $X = \\{1,2,3,...\\}$, we need to start by working out $p_{x^n}$.\n",
    "\n",
    "To get $X=n$ you need to throw $n-1$ tails, for which the probability is $\\displaystyle\\frac{1}{2^{n-1}}$ followed by a head which has the probability of $\\displaystyle\\frac{1}{2}$, and therefore $p_x(n)=\\displaystyle\\frac{1}{2^n}$\n",
    "\n",
    "To calculate entropy, use the formula:\n",
    "\n",
    "$H(X) = \\displaystyle-\\sum_{n\\in\\mathcal{X}}\\frac{1}{2^n}\\log{2^{-n}}$\n",
    "\n",
    "Which cancels to\n",
    "\n",
    "$\\displaystyle\\sum_{n=1}^\\infty\\frac{n}{2^n} = \\sum_{n=1}^\\infty\\frac{n}{\\frac{1}{1-2}}=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b.\n",
    "\n",
    "Since you can ask the question \"is it $n$?\" aka the outcome you are after, this also has a $\\frac{1}{2}$ chance of being correct so the average number of questions would also be 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: A puzzle which lends itself to information type reasoning\n",
    "\n",
    "Suppose you have $n$ coins, among which there may or may not be one counterfit coin. If there is a counterfit coin it will either way less or more than the other coins. The coins are weighted using a balance, any number of coins can be put on each side of the balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are $2n+1$ possibilities that the coins can be arranged in\n",
    "\n",
    "$\\{R_1, R_2, R_3, ..., R_N\\}$\n",
    "\n",
    "$\\{F_1, R_2, R_3, ..., R_N\\}$\n",
    "\n",
    "$\\{R_1, F_2, R_3, ..., R_N\\}$\n",
    "\n",
    "$\\{R_1, R_2, F_3, ..., R_N\\}$\n",
    "\n",
    "....\n",
    "\n",
    "$\\{R_1, R_2, R_3, ..., F_N\\}$\n",
    "\n",
    "Since the coins can either be heavier or lighter it is $2\\times$\n",
    "\n",
    "If you work out the entropy\n",
    "\n",
    "$H(X) = -\\displaystyle2n+1\\frac{1}{2n+1}\\log\\frac{1}{2n+1} $\n",
    "\n",
    "if you simplyify this you get\n",
    "\n",
    "$H(X) = \\log{2n+1}$\n",
    "\n",
    "For weighing stratagies, $H(Y)$ there are three possible outcomes, it is even, it tips left or right, therefore the probability is $\\frac{1}{3}$\n",
    "\n",
    "$H(Y) = 3\\times-\\displaystyle\\frac{1}{3}\\log\\frac{1}{3} = \\log3$\n",
    "\n",
    "Since weighing each coin individually (taking $2n+1$ steps) and having an effective weighing stratagy gives you the same information $H(X)= H(Y)$\n",
    "\n",
    "But our weighing stratagy should take less time (on average) than an exaustive search and therefore\n",
    "\n",
    "$H(X) \\leq k\\log3$\n",
    "\n",
    "where $k$ is the number of weighs performed. Now substitute in $H(X)$ and:\n",
    "\n",
    "$\\displaystyle\\log2n+1 \\leq k\\log3$\n",
    "\n",
    "$\\displaystyle\\log2n+1 \\leq \\log3^k$\n",
    "\n",
    "$\\displaystyle 2n+1 \\leq 3^k$\n",
    "\n",
    "$\\displaystyle n \\leq \\frac{3^k-1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b.\n",
    "\n",
    "\n",
    "Using $k=3$ and $12$ coins, what is the best weighing stratagy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coin():\n",
    "    \n",
    "    def __init__(self, weight: int = 2):\n",
    "        self.weight = weight\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.weight)\n",
    "\n",
    "class Coins():\n",
    "    \n",
    "    def __init__(self, index: int, coins: int = 12):\n",
    "        self.index = index\n",
    "        self.coins = [Coin() for i in range(1,coins+1)]\n",
    "        self._initialise_weights()\n",
    "    \n",
    "    def _initialise_weights(self):\n",
    "        self.coins[self.index].weight = 3\n",
    "        \n",
    "coins = Coins(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Working out entropy and information\n",
    "\n",
    "Let $p(x,y)$ be given by $p(0,0)=p(0,1)=p(1,1)=\\displaystyle\\frac{1}{3}$ and $p(1,0)=0$:\n",
    "\n",
    "| Y\\X | 0 | 1 |\n",
    "| ---- | ---- | ---- |\n",
    "| 0 | $\\frac{1}{3}$ | 0 |\n",
    "| 1 | $\\frac{1}{3}$ | $\\frac{1}{3}$ |\n",
    "\n",
    "Find $H(X), H(Y), H(X\\,|\\,Y), H(Y\\,|\\,X), H(X, Y), H(Y) - H(Y\\,|\\,X)$ and $I(X;Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distribution\n",
    "\n",
    "P = np.array([\n",
    "    [1/3, 0],\n",
    "    [1/3, 1/3]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H(X)\n",
    "\n",
    "entropy(P.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H(Y)\n",
    "\n",
    "entropy(P.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666665"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H(X|Y)\n",
    "\n",
    "conditional_entropy(P, P.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666665"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H(Y|X)\n",
    "\n",
    "conditional_entropy(P, P.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.584962500721156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H(X,Y)\n",
    "\n",
    "joint_entropy(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25162916738782304"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H(Y) - H(Y|X)\n",
    "\n",
    "entropy(P.sum(1)) - conditional_entropy(P, P.sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5: A qeustion about information in the brain\n",
    "\n",
    "The original idea of estimating neural information by binning spike trains was spread across several papers, but one of the main references is ... One aspect of this paper we didn't discuss is the use of extrapolation to estimate the information as the number of samples becomes large based on the behaviour for smaller numbers of samples.\n",
    "\n",
    "Can you give a short, up to five line, summary of what this involves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
